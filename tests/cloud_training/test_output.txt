============================= test session starts ==============================
platform linux -- Python 3.10.6, pytest-7.3.1, pluggy-1.0.0 -- /home/lewagon/.pyenv/versions/3.10.6/envs/taxifare-env/bin/python3.10
cachedir: .pytest_cache
rootdir: /home/lewagon/code/maspriv/data-train-in-the-cloud/tests
configfile: pytest_kitt.ini
collecting ... collected 9 items

tests/cloud_training/test_cloud_data.py::TestCloudData::test_big_query_dataset_variable_exists PASSED [ 11%]
tests/cloud_training/test_cloud_data.py::TestCloudData::test_cloud_data_create_dataset PASSED [ 22%]
tests/cloud_training/test_cloud_data.py::TestCloudData::test_cloud_data_create_table PASSED [ 33%]
tests/cloud_training/test_main.py::TestMain::test_route_preprocess FAILED [ 44%]
tests/cloud_training/test_main.py::TestMain::test_route_train[local] PASSED [ 55%]
tests/cloud_training/test_main.py::TestMain::test_route_train[gcs] PASSED [ 66%]
tests/cloud_training/test_main.py::TestMain::test_route_evaluate PASSED  [ 77%]
tests/cloud_training/test_main.py::TestMain::test_route_pred PASSED      [ 88%]
tests/cloud_training/test_vm.py::test_i_am_a_vm FAILED                   [100%]

=================================== FAILURES ===================================
________________________ TestMain.test_route_preprocess ________________________

self = <tests.cloud_training.test_main.TestMain object at 0x7f69002609d0>
fixture_query_1k =      fare_amount           pickup_datetime  ...  dropoff_latitude  passenger_count
0            8.9 2009-01-15 09:22:3...           4
454          8.5 2014-12-27 16:47:42+00:00  ...         40.771263                4

[455 rows x 7 columns]

    def test_route_preprocess(self, fixture_query_1k):
    
        from taxifare.interface.main import preprocess
    
        data_query_path = Path(LOCAL_DATA_PATH).joinpath("raw",f"query_{MIN_DATE}_{MAX_DATE}_{DATA_SIZE}.csv")
        data_processed_path = Path(LOCAL_DATA_PATH).joinpath("processed",f"processed_{MIN_DATE}_{MAX_DATE}_{DATA_SIZE}.csv")
    
        data_query_exists = data_query_path.is_file()
        data_processed_exists = data_processed_path.is_file()
    
        # SETUP
        if data_query_exists:
            shutil.copyfile(data_query_path, f'{data_query_path}_backup')
            data_query_path.unlink()
        if data_processed_exists:
            shutil.copyfile(data_processed_path, f'{data_processed_path}_backup')
            data_processed_path.unlink()
    
        # ACT
        # TODO: add try-except to be certain of reseting state if crash
        # Check route runs querying Big Query
        preprocess(min_date=MIN_DATE, max_date=MAX_DATE)
    
        # Load newly saved query data and test it against true fixture
>       data_query = pd.read_csv(data_query_path, parse_dates=["pickup_datetime"])

tests/cloud_training/test_main.py:46: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../.pyenv/versions/3.10.6/envs/taxifare-env/lib/python3.10/site-packages/pandas/util/_decorators.py:211: in wrapper
    return func(*args, **kwargs)
../../../.pyenv/versions/3.10.6/envs/taxifare-env/lib/python3.10/site-packages/pandas/util/_decorators.py:331: in wrapper
    return func(*args, **kwargs)
../../../.pyenv/versions/3.10.6/envs/taxifare-env/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950: in read_csv
    return _read(filepath_or_buffer, kwds)
../../../.pyenv/versions/3.10.6/envs/taxifare-env/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605: in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
../../../.pyenv/versions/3.10.6/envs/taxifare-env/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442: in __init__
    self._engine = self._make_engine(f, self.engine)
../../../.pyenv/versions/3.10.6/envs/taxifare-env/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1735: in _make_engine
    self.handles = get_handle(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

path_or_buf = PosixPath('/home/lewagon/.lewagon/mlops/data/raw/query_2009-01-01_2015-01-01_1k.csv')
mode = 'r'

    @doc(compression_options=_shared_docs["compression_options"] % "path_or_buf")
    def get_handle(
        path_or_buf: FilePath | BaseBuffer,
        mode: str,
        *,
        encoding: str | None = None,
        compression: CompressionOptions = None,
        memory_map: bool = False,
        is_text: bool = True,
        errors: str | None = None,
        storage_options: StorageOptions = None,
    ) -> IOHandles[str] | IOHandles[bytes]:
        """
        Get file handle for given path/buffer and mode.
    
        Parameters
        ----------
        path_or_buf : str or file handle
            File path or object.
        mode : str
            Mode to open path_or_buf with.
        encoding : str or None
            Encoding to use.
        {compression_options}
    
            .. versionchanged:: 1.0.0
               May now be a dict with key 'method' as compression mode
               and other keys as compression options if compression
               mode is 'zip'.
    
            .. versionchanged:: 1.1.0
               Passing compression options as keys in dict is now
               supported for compression modes 'gzip', 'bz2', 'zstd' and 'zip'.
    
            .. versionchanged:: 1.4.0 Zstandard support.
    
        memory_map : bool, default False
            See parsers._parser_params for more information. Only used by read_csv.
        is_text : bool, default True
            Whether the type of the content passed to the file/buffer is string or
            bytes. This is not the same as `"b" not in mode`. If a string content is
            passed to a binary file/buffer, a wrapper is inserted.
        errors : str, default 'strict'
            Specifies how encoding and decoding errors are to be handled.
            See the errors argument for :func:`open` for a full list
            of options.
        storage_options: StorageOptions = None
            Passed to _get_filepath_or_buffer
    
        .. versionchanged:: 1.2.0
    
        Returns the dataclass IOHandles
        """
        # Windows does not default to utf-8. Set to utf-8 for a consistent behavior
        encoding = encoding or "utf-8"
    
        errors = errors or "strict"
    
        # read_csv does not know whether the buffer is opened in binary/text mode
        if _is_binary_mode(path_or_buf, mode) and "b" not in mode:
            mode += "b"
    
        # validate encoding and errors
        codecs.lookup(encoding)
        if isinstance(errors, str):
            codecs.lookup_error(errors)
    
        # open URLs
        ioargs = _get_filepath_or_buffer(
            path_or_buf,
            encoding=encoding,
            compression=compression,
            mode=mode,
            storage_options=storage_options,
        )
    
        handle = ioargs.filepath_or_buffer
        handles: list[BaseBuffer]
    
        # memory mapping needs to be the first step
        # only used for read_csv
        handle, memory_map, handles = _maybe_memory_map(handle, memory_map)
    
        is_path = isinstance(handle, str)
        compression_args = dict(ioargs.compression)
        compression = compression_args.pop("method")
    
        # Only for write methods
        if "r" not in mode and is_path:
            check_parent_directory(str(handle))
    
        if compression:
            if compression != "zstd":
                # compression libraries do not like an explicit text-mode
                ioargs.mode = ioargs.mode.replace("t", "")
            elif compression == "zstd" and "b" not in ioargs.mode:
                # python-zstandard defaults to text mode, but we always expect
                # compression libraries to use binary mode.
                ioargs.mode += "b"
    
            # GZ Compression
            if compression == "gzip":
                if isinstance(handle, str):
                    # error: Incompatible types in assignment (expression has type
                    # "GzipFile", variable has type "Union[str, BaseBuffer]")
                    handle = gzip.GzipFile(  # type: ignore[assignment]
                        filename=handle,
                        mode=ioargs.mode,
                        **compression_args,
                    )
                else:
                    handle = gzip.GzipFile(
                        # No overload variant of "GzipFile" matches argument types
                        # "Union[str, BaseBuffer]", "str", "Dict[str, Any]"
                        fileobj=handle,  # type: ignore[call-overload]
                        mode=ioargs.mode,
                        **compression_args,
                    )
    
            # BZ Compression
            elif compression == "bz2":
                # No overload variant of "BZ2File" matches argument types
                # "Union[str, BaseBuffer]", "str", "Dict[str, Any]"
                handle = bz2.BZ2File(  # type: ignore[call-overload]
                    handle,
                    mode=ioargs.mode,
                    **compression_args,
                )
    
            # ZIP Compression
            elif compression == "zip":
                # error: Argument 1 to "_BytesZipFile" has incompatible type
                # "Union[str, BaseBuffer]"; expected "Union[Union[str, PathLike[str]],
                # ReadBuffer[bytes], WriteBuffer[bytes]]"
                handle = _BytesZipFile(
                    handle, ioargs.mode, **compression_args  # type: ignore[arg-type]
                )
                if handle.buffer.mode == "r":
                    handles.append(handle)
                    zip_names = handle.buffer.namelist()
                    if len(zip_names) == 1:
                        handle = handle.buffer.open(zip_names.pop())
                    elif not zip_names:
                        raise ValueError(f"Zero files found in ZIP file {path_or_buf}")
                    else:
                        raise ValueError(
                            "Multiple files found in ZIP file. "
                            f"Only one file per ZIP: {zip_names}"
                        )
    
            # TAR Encoding
            elif compression == "tar":
                compression_args.setdefault("mode", ioargs.mode)
                if isinstance(handle, str):
                    handle = _BytesTarFile(name=handle, **compression_args)
                else:
                    # error: Argument "fileobj" to "_BytesTarFile" has incompatible
                    # type "BaseBuffer"; expected "Union[ReadBuffer[bytes],
                    # WriteBuffer[bytes], None]"
                    handle = _BytesTarFile(
                        fileobj=handle, **compression_args  # type: ignore[arg-type]
                    )
                assert isinstance(handle, _BytesTarFile)
                if "r" in handle.buffer.mode:
                    handles.append(handle)
                    files = handle.buffer.getnames()
                    if len(files) == 1:
                        file = handle.buffer.extractfile(files[0])
                        assert file is not None
                        handle = file
                    elif not files:
                        raise ValueError(f"Zero files found in TAR archive {path_or_buf}")
                    else:
                        raise ValueError(
                            "Multiple files found in TAR archive. "
                            f"Only one file per TAR archive: {files}"
                        )
    
            # XZ Compression
            elif compression == "xz":
                # error: Argument 1 to "LZMAFile" has incompatible type "Union[str,
                # BaseBuffer]"; expected "Optional[Union[Union[str, bytes, PathLike[str],
                # PathLike[bytes]], IO[bytes]]]"
                handle = get_lzma_file()(handle, ioargs.mode)  # type: ignore[arg-type]
    
            # Zstd Compression
            elif compression == "zstd":
                zstd = import_optional_dependency("zstandard")
                if "r" in ioargs.mode:
                    open_args = {"dctx": zstd.ZstdDecompressor(**compression_args)}
                else:
                    open_args = {"cctx": zstd.ZstdCompressor(**compression_args)}
                handle = zstd.open(
                    handle,
                    mode=ioargs.mode,
                    **open_args,
                )
    
            # Unrecognized Compression
            else:
                msg = f"Unrecognized compression type: {compression}"
                raise ValueError(msg)
    
            assert not isinstance(handle, str)
            handles.append(handle)
    
        elif isinstance(handle, str):
            # Check whether the filename is to be opened in binary mode.
            # Binary mode does not support 'encoding' and 'newline'.
            if ioargs.encoding and "b" not in ioargs.mode:
                # Encoding
>               handle = open(
                    handle,
                    ioargs.mode,
                    encoding=ioargs.encoding,
                    errors=errors,
                    newline="",
                )
E               FileNotFoundError: [Errno 2] No such file or directory: '/home/lewagon/.lewagon/mlops/data/raw/query_2009-01-01_2015-01-01_1k.csv'

../../../.pyenv/versions/3.10.6/envs/taxifare-env/lib/python3.10/site-packages/pandas/io/common.py:856: FileNotFoundError
----------------------------- Captured stdout call -----------------------------
[34m
Loading TensorFlow...[0m

✅ TensorFlow loaded (1.52s)
[35m
 ⭐️ Use case: preprocess[0m
[34m
Load data from local CSV...[0m
✅ Data loaded, with shape (90743, 7)
✅ data cleaned
[34m
Preprocessing features...[0m
✅ X_processed, with shape (88381, 65)
[34m
Save data to BigQuery @ strong-province-388216.taxifare.processed_200k...:[0m

Write strong-province-388216.taxifare.processed_200k (88381 rows)
✅ Data saved to bigquery, with shape (88381, 67)
✅ preprocess() done 

----------------------------- Captured stderr call -----------------------------
2023-06-03 11:31:30.609411: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-03 11:31:30.710876: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2023-06-03 11:31:30.710899: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2023-06-03 11:31:30.734533: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-06-03 11:31:31.355139: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2023-06-03 11:31:31.355255: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2023-06-03 11:31:31.355266: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
________________________________ test_i_am_a_vm ________________________________

    def test_i_am_a_vm():
        """
        Test that this code is being run from a Google VM named as per env variable 'INSTANCE'
        """
    
>       assert platform.node() == INSTANCE, f"You should be running from your instance named '{INSTANCE}'."
E       AssertionError: You should be running from your instance named 'taxi-instance'.
E       assert 'Massimo' == 'taxi-instance'
E         - taxi-instance
E         + Massimo

tests/cloud_training/test_vm.py:9: AssertionError
=========================== short test summary info ============================
FAILED tests/cloud_training/test_main.py::TestMain::test_route_preprocess - F...
FAILED tests/cloud_training/test_vm.py::test_i_am_a_vm - AssertionError: You ...
============= 2 failed, 7 passed, 18 warnings in 71.04s (0:01:11) ==============
